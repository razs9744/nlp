{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cupy as cp\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = 999\n",
    "np.random.seed(s)\n",
    "spacy.util.fix_random_seed(s)\n",
    "ner=nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INDICA', 'Afghani', 'Afgoo', 'Berry White', 'Blueberry', 'Bubba Kush', 'G13', 'Granddaddy Purple', 'Grape Ape', 'Herijuana', 'Hindu Kush', 'Ingrid', 'Kosher Kush', 'Lavender', 'Master Kush', 'Northern Lights', 'Obama Kush', 'Pez', 'Plushberry', 'Presidential OG', 'Purple Urkle', 'Willy’s Wonder', 'HYBRID', 'ACDC', 'AK-47', 'Zkittlez', 'Ewok', 'Gelato', 'Banana OG', 'Blue Dream', 'Cannatonic', 'Chemdawg', 'Chernobyl', 'Cherry Pie', 'Cinderella 99', 'Dancehall', 'Double Dream', 'Dutch Treat', 'Fruity Pebbles', 'Headband', 'Jean Guy', 'Jillybean', 'Juicy Fruit', 'Larry OG', 'Lemonder', 'Lodi Dodi', 'Mango Kush', 'Mendocino Purps', 'Middlefork', 'OG Kush', 'Pineapple Chunk', 'Pineapple Express', 'Pink Kush', 'Raskal OG', 'SAGE', 'SFV OG', 'Shiatsu Kush', 'Skunk No. 1', 'Snoop’s Dream', 'Snowcap', 'Sour OG', 'Sour Tsunami', 'Space Queen', 'Sunset Sherbet', 'Tahoe OG', 'Tangerine Dream', 'Trainwreck', 'UK Cheese', 'White Fire OG', 'White Widow', 'XJ-13', 'SATIVA', 'Acapulco Gold', 'Alaskan Thunder Fuck', 'Allen Wrench', 'Amnesia', 'Bay 11', 'Chocolope', 'Cinex', 'Dirty Girl', 'Durban Poison', 'Ghost Train Haze', 'Grapefruit', 'Green Crack', 'Harlequin', 'Island Sweet Skunk', 'Jack Herer', 'Kali Mist', 'Lamb’s Bread', 'Laughing Buddha', 'Maui Wowie', 'Panama Red', 'Purple Haze', 'Red Headed Stranger', 'Schrom', 'Sour Diesel', 'Strawberry Cough', 'Super Lemon Haze', 'Super Silver Haze', 'Tangie']\n"
     ]
    }
   ],
   "source": [
    "f = open('plain_cannabis.txt', 'r')\n",
    "strains = f.read().splitlines()\n",
    "text = ' '.join(strains)\n",
    "print(strains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "\n",
    "for strain in strains:\n",
    "    split_strain = strain.split()\n",
    "    if len(split_strain) > 1:\n",
    "        for i,word in enumerate(split_strain, 1):\n",
    "            if i == 1:\n",
    "                words.append(word)\n",
    "                labels.append('B-STRAIN')\n",
    "            elif i ==len(split_strain):\n",
    "                words.append(word)\n",
    "                labels.append('L-STRAIN')\n",
    "            else:\n",
    "                words.append(word)\n",
    "                labels.append('I-STRAIN')\n",
    "    else:  \n",
    "        words.append(strain)\n",
    "        labels.append('U-STRAIN') # As most of token will be non-entity (OUT). Replace this later with actual entity a/c the scheme.\n",
    "\n",
    "df = pd.DataFrame({'word': words, 'label': labels})\n",
    "df.to_csv('cannabis_data.bilou', index=False) # biluo in extension to indicate the type of encoding, it is ok to keep csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dpath = 'cannabis_data_e.bilou'\n",
    "df = pd.read_csv(dpath, sep=',')\n",
    "words  = list(df.word.values)\n",
    "ents = list(df.label.values)\n",
    "text = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_ents = ['STRAIN'] #\n",
    "\n",
    "prev_ents = ner.move_names\n",
    "\n",
    "for ent in add_ents:\n",
    "    ner.add_label(ent)\n",
    "    \n",
    "new_ents = ner.move_names\n",
    "# print('\\n[All Entities] = ', ner.move_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDICA Afghani Afgoo Berry White Blueberry Bubba Kush G13 Granddaddy Purple Grape Ape Herijuana Hindu Kush Ingrid Kosher Kush Lavender Master Kush Northern Lights Obama Kush Pez Plushberry Presidential OG Purple Urkle HYBRID ACDC Zkittlez Ewok Gelato Banana OG Blue Dream Cannatonic Chemdawg Chernobyl Cherry Pie Cinderella 99 Dancehall Double Dream Dutch Treat Fruity Pebbles Headband Jean Guy Jillybean Juicy Fruit Larry OG Lemonder Lodi Dodi Mango Kush Mendocino Purps Middlefork OG Kush Pineapple Chunk Pineapple Express Pink Kush Raskal OG SAGE SFV OG Shiatsu Kush Snowcap Sour OG Sour Tsunami Space Queen Sunset Sherbet Tahoe OG Tangerine Dream Trainwreck UK Cheese White Fire OG White Widow SATIVA Acapulco Gold Alaskan Thunder Fuck Allen Wrench Amnesia Bay 11 Chocolope Cinex Dirty Girl Durban Poison Ghost Train Haze Grapefruit Green Crack Harlequin Island Sweet Skunk Jack Herer Kali Mist Laughing Buddha Maui Wowie Panama Red Purple Haze Red Headed Stranger Schrom Sour Diesel Strawberry Cough Super Lemon Haze Super Silver Haze Tangie\n"
     ]
    }
   ],
   "source": [
    "#### Create Dataset\n",
    "from spacy.training import Example\n",
    "print(text)\n",
    "doc = nlp.make_doc(text)\n",
    "g = Example.from_dict(doc, {\"entities\": ents})\n",
    "# Add examples as avaialble or needed\n",
    "X = [doc]\n",
    "Y = [ g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OtherPipes] = ['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'] will be disabled\n"
     ]
    }
   ],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "print(f'[OtherPipes] = {other_pipes} will be disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "              (\"Indica remains the most common form of the drug.\", {\"entities\": [(0, 6, \"STAIN\")]}),\n",
    "              (\"The English word canvas sufficiently reveals its derivation from cannabis.\", {\"entities\": [(66, 75, \"STAIN\")]}),\n",
    "              (\"ample evidence to suggest sativa be legal.\", {\"entities\": [(27,34, \"STAIN\")]}),\n",
    "              (\"Lavender downgraded from a Class B to Class C drug.\", {\"entities\": [(0,8, \"STAIN\")]}),\n",
    "              (\"The price for good herbal lemonder should be £ 120 an ounce.\", {\"entities\": [(27,36, \"STAIN\")]}),\n",
    "              (\"What is GW's position on crude herbal g13?\", {\"entities\": [(39,42, \"STAIN\")]}),\n",
    "              (\"cannabis blueberry.\", {\"entities\": [(9,18, \"STAIN\")]})\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inbal/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"The English word canvas sufficiently reveals its d...\" with entities \"[(66, 75, 'STAIN')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/inbal/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"ample evidence to suggest sativa be legal.\" with entities \"[(27, 34, 'STAIN')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/inbal/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"The price for good herbal lemonder should be £ 120...\" with entities \"[(27, 36, 'STAIN')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/inbal/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"What is GW's position on crude herbal g13?\" with entities \"[(39, 42, 'STAIN')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "examples = []\n",
    "for text, annots in TRAIN_DATA:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "nlp.initialize(lambda: examples)\n",
    "for i in range(20):\n",
    "    random.shuffle(examples)\n",
    "    for batch in spacy.util.minibatch(examples, size=8):\n",
    "        nlp.update(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'transformer': 85.78706187009811, 'tagger': 0.0, 'parser': 0.0, 'ner': 26.91504728794098}\n",
      "\n",
      "Losses {'transformer': 144.1557461619377, 'tagger': 0.0, 'parser': 0.0, 'ner': 53.22083252668381}\n",
      "\n",
      "Losses {'transformer': 206.93320339918137, 'tagger': 0.0, 'parser': 0.0, 'ner': 87.48028248548508}\n",
      "\n",
      "Losses {'transformer': 258.3176002204418, 'tagger': 0.0, 'parser': 0.0, 'ner': 106.59361857175827}\n",
      "\n",
      "Losses {'transformer': 297.2268190085888, 'tagger': 0.0, 'parser': 0.0, 'ner': 137.0761826634407}\n",
      "\n",
      "Losses {'transformer': 304.23779578506947, 'tagger': 0.0, 'parser': 0.0, 'ner': 159.87776166200638}\n",
      "\n",
      "Losses {'transformer': 328.80488486588, 'tagger': 0.0, 'parser': 0.0, 'ner': 193.06311017274857}\n",
      "\n",
      "Losses {'transformer': 340.59306724369526, 'tagger': 0.0, 'parser': 0.0, 'ner': 215.0660789012909}\n",
      "\n",
      "Losses {'transformer': 419.6733380109072, 'tagger': 0.0, 'parser': 0.0, 'ner': 252.37887692451477}\n",
      "\n",
      "Losses {'transformer': 429.99520452320576, 'tagger': 0.0, 'parser': 0.0, 'ner': 270.2199483513832}\n",
      "\n",
      "Losses {'transformer': 457.25942762196064, 'tagger': 0.0, 'parser': 0.0, 'ner': 296.60158973932266}\n",
      "\n",
      "Losses {'transformer': 526.7367473393679, 'tagger': 0.0, 'parser': 0.0, 'ner': 323.04443168640137}\n",
      "\n",
      "Losses {'transformer': 541.3658892661333, 'tagger': 0.0, 'parser': 0.0, 'ner': 355.3424143791199}\n",
      "\n",
      "Losses {'transformer': 616.1161871701479, 'tagger': 0.0, 'parser': 0.0, 'ner': 376.06451511383057}\n",
      "\n",
      "Losses {'transformer': 644.6252334862947, 'tagger': 0.0, 'parser': 0.0, 'ner': 408.05412274599075}\n",
      "\n",
      "Losses {'transformer': 699.8099622279406, 'tagger': 0.0, 'parser': 0.0, 'ner': 426.6315008401871}\n",
      "\n",
      "Losses {'transformer': 789.2860388308764, 'tagger': 0.0, 'parser': 0.0, 'ner': 462.49282455444336}\n",
      "\n",
      "Losses {'transformer': 800.8179581910372, 'tagger': 0.0, 'parser': 0.0, 'ner': 482.14511585235596}\n",
      "\n",
      "Losses {'transformer': 825.794021114707, 'tagger': 0.0, 'parser': 0.0, 'ner': 509.9739787578583}\n",
      "\n",
      "Losses {'transformer': 887.785893663764, 'tagger': 0.0, 'parser': 0.0, 'ner': 536.2015926837921}\n",
      "\n",
      "Losses {'transformer': 955.4375722259283, 'tagger': 0.0, 'parser': 0.0, 'ner': 568.2288506031036}\n",
      "\n",
      "Losses {'transformer': 976.1339409798384, 'tagger': 0.0, 'parser': 0.0, 'ner': 589.317745089531}\n",
      "\n",
      "Losses {'transformer': 1016.6677700728178, 'tagger': 0.0, 'parser': 0.0, 'ner': 617.2159135341644}\n",
      "\n",
      "Losses {'transformer': 1048.9849650114775, 'tagger': 0.0, 'parser': 0.0, 'ner': 641.4068802595139}\n",
      "\n",
      "Losses {'transformer': 1080.6409872025251, 'tagger': 0.0, 'parser': 0.0, 'ner': 664.1256208643317}\n",
      "\n",
      "Losses {'transformer': 1148.1923959702253, 'tagger': 0.0, 'parser': 0.0, 'ner': 693.005999289453}\n",
      "\n",
      "Losses {'transformer': 1169.5691786557436, 'tagger': 0.0, 'parser': 0.0, 'ner': 722.617608986795}\n",
      "\n",
      "Losses {'transformer': 1242.4658351689577, 'tagger': 0.0, 'parser': 0.0, 'ner': 747.8109008893371}\n",
      "\n",
      "Losses {'transformer': 1311.2795503884554, 'tagger': 0.0, 'parser': 0.0, 'ner': 776.16829123348}\n",
      "\n",
      "Losses {'transformer': 1373.0738118439913, 'tagger': 0.0, 'parser': 0.0, 'ner': 804.3533655628562}\n",
      "\n",
      "Losses {'transformer': 1492.0956319123507, 'tagger': 0.0, 'parser': 0.0, 'ner': 837.5997798070312}\n",
      "\n",
      "Losses {'transformer': 1498.019438073039, 'tagger': 0.0, 'parser': 0.0, 'ner': 858.7458667382598}\n",
      "\n",
      "Losses {'transformer': 1512.1300743073225, 'tagger': 0.0, 'parser': 0.0, 'ner': 888.5116741284728}\n",
      "\n",
      "Losses {'transformer': 1578.355622574687, 'tagger': 0.0, 'parser': 0.0, 'ner': 914.4843972548842}\n",
      "\n",
      "Losses {'transformer': 1655.0919907540083, 'tagger': 0.0, 'parser': 0.0, 'ner': 945.2667642459273}\n",
      "\n",
      "Losses {'transformer': 1692.4804258793592, 'tagger': 0.0, 'parser': 0.0, 'ner': 962.9542874321342}\n",
      "\n",
      "Losses {'transformer': 1783.0403681844473, 'tagger': 0.0, 'parser': 0.0, 'ner': 994.2767647132277}\n",
      "\n",
      "Losses {'transformer': 1799.7225862592459, 'tagger': 0.0, 'parser': 0.0, 'ner': 1018.311653457582}\n",
      "\n",
      "Losses {'transformer': 1866.2046816200018, 'tagger': 0.0, 'parser': 0.0, 'ner': 1051.078469954431}\n",
      "\n",
      "Losses {'transformer': 1872.3602785915136, 'tagger': 0.0, 'parser': 0.0, 'ner': 1075.3111061677337}\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "for text, annots in TRAIN_DATA:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "nlp.initialize(lambda: examples)\n",
    "losses = {}\n",
    "\n",
    "for i in range(20):\n",
    "    random.shuffle(examples)\n",
    "    for batch in spacy.util.minibatch(examples, size=4):\n",
    "        print()\n",
    "        nlp.update(batch,                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses)\n",
    "        print(\"Losses\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-03-21 00:20:07,543] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'cannabis'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-03-21 00:20:07,544] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'sativa'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannabis sativa DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('cannabis blueberry.')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (nlp)",
   "language": "python",
   "name": "pycharm-183e1216"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
